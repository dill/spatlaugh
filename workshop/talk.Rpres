Spatial modelling etc
=====================
css: custom.css

David L Miller & Mark V Bravington

International Whaling Commission Scientific Committee 2017

```{r setup, include=FALSE}
library(knitr)
library(viridis)
opts_chunk$set(cache=TRUE, echo=FALSE)

# some useful libraries
library(dsm)
library(mgcv)

library(tweedie)
library(RColorBrewer)

# load the sperm whale data
load("spermwhale-analysis/df-models.RData")
load("spermwhale-analysis/count-models.RData")
load("spermwhale-analysis/predgrid.RData")
load("spermwhale-analysis/sperm-data.RData")

# make a theme
library(ggplot2)
library(cowplot)
sp_theme <- theme_cowplot() +
  theme(panel.grid.major = element_line(colour="grey92", size=0.25),
        axis.line=element_blank())

# add the US to maps
library(maptools)
library(rgeos)
library(maps)
library(mapdata)
coastline <- map("world", c("USA"), fill=TRUE, plot=FALSE)
coastline <- map2SpatialPolygons(coastline, ID=coastline$name,# ID=IDs,
                                 proj4string=CRS("+proj=longlat +datum=WGS84"))
# AEA projection, as in EEZ models of Roberts et al
Projection <- CRS("+proj=aea +lat_1=38 +lat_2=30 +lat_0=34 +lon_0=-73 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0")
# project!
coastline <- spTransform(coastline, Projection)
bnd <- as.data.frame(coastline@polygons[[1]]@Polygons[[1]]@coords)
colnames(bnd) <- c("x", "y")

us <- geom_polygon(aes(x=x, y=y), fill = "#1A9850",
               data=bnd)
```

Why are we here/why did we do this?
===================================

- Stratified Horvitz-Thompson is workhorse of many abundance estimates
- How is H-T going to fail?
- When do we need to use spatial models?
- What are "Bad surveys"?
- Report [http://converged.yt/papers/iwc-2017-ht.pdf](http://converged.yt/papers/iwc-2017-ht.pdf)


Overview
========

- Today:
  1. what's wrong with H-T?
  2. Spatial models overview
  3. What can go wrong with spatial models?
  4. Testing designs in R
  5. Model checking for spatial models
- Tomorrow:
  1. What we missed, what is hard
  2. Try out tester on your data
  3. Other methods/software, future work
  5. Guidelines

Practicalities
==============

- Try not to talk for more than an hour without a break
- We both have funny accents, yell if you don't understand!
- There is maths -- don't worry

This is not a distance sampling course!
======================================

- This material usually takes 4 days+ to teach
- This will not prepare you to analyse spatial data
- BUT you can do this in St Andrews this summer!
- [creem2.st-andrews.ac.uk](http://creem2.st-andrews.ac.uk)

<img src="images/observatory.jpg" width="45%"> <img src="images/creem.png" width="40%">

Why are we interested in spatially-explicit estimation?
======================
type:section

Inferential aims
================

<img src="images/nvsmap.png" width="100%">

blah
=========
type:section
title:none

<p style="font-size:800%">Part I</p>



Horvitz-Thompson estimation: the good, the bad and the ugly
===========================
type:section


Horvitz-Thompson-like estimators
========================================================

- Rescale the (flat) density and extrapolate

$$
\hat{N} = \frac{\text{study area}}{\text{covered area}}\sum_{i=1}^n \frac{s_i}{\hat{p}_i}
$$

- $s_i$ are group/cluster sizes
- $\hat{p}_i$ is the detection probability (from distance sampling)

Variance of H-T
===============

- Multiple sources of randomness in H-T equation:
  - $\hat{p}_i$ - detectability
  - $n$ - dealt with as $n/L$, encounter rate
  - $s$ - group size



Hidden in this formula is a simple assumption
=============================================

- Probability of sampling every point in the study area is equal
- Is this true? Sometimes.
- If (and only if) the design is randomised

Many faces of randomisation
===========================

```{r randomisation, fig.width=14, fig.height=4.5, out.width='\\textwidth'}
set.seed(12133)
par(mfrow=c(1,3), cex.main=2.5)

# true random sample
plot(c(0, 1), c(0, 1), type="n", xlab="", ylab="", axes=FALSE, asp=1, main="random placement")
dat <- data.frame(x=runif(10), y=runif(10))
angle <- runif(10, 0, 2*pi)
len <- 0.2
arrows(dat$x, dat$y, dat$x+len*cos(angle), dat$y+len*sin(angle), length=0)
dat <- data.frame(x=runif(10), y=runif(10))
angle <- runif(10, 0, 2*pi)
len <- 0.2
arrows(dat$x, dat$y, dat$x+len*cos(angle), dat$y+len*sin(angle), length=0, col="grey40", lty=2)
box()

# parallel random offset
plot(c(0, 1), c(0, 1), type="n", xlab="", ylab="", axes=FALSE, asp=1, main="random offset parallel lines")
abline(v=seq(0, 1, len=10))
abline(v=seq(0, 1, len=10)+0.07, col="grey40", lty=2)
box()

# random offset zigzag

## make a zigzag
n_segs <- 10
zz <- data.frame(x   = c(seq(0, 0.5, len=n_segs),
                         seq(0.5, 1, len=n_segs)),
                 y   = c(seq(0, 1, len=n_segs),
                         seq(1, 0, len=n_segs)))
# many zigzags
mzz <- rbind(zz,zz,zz)
mzz$x <- mzz$x/3
ind <- 1:nrow(zz)
mzz$x[ind+nrow(zz)] <- mzz$x[ind+nrow(zz)]+1/3
mzz$x[ind+2*nrow(zz)] <- mzz$x[ind+2*nrow(zz)]+2/3

plot(mzz, type="l", xlab="", ylab="", axes=FALSE, asp=1, main="random offset zigzag")
lines(mzz$x+0.06, mzz$y, col="grey40", lty=2)
box()
```

What does this randomisation give us?
=====================================

- Coverage probability
- H-T estimator assumes even coverage
- (or you can estimate)
- Otherwise not really valid

Estimating coverage
===================

- We can estimate coverage of a non-uniform design!
- In Distance!
- Example from BC, Canada in this paper:

![Thomas paper](images/thomas-paper.png)


Estimating coverage
===================

![](images/bad_coverage.png)

A complex survey plan
=====================

![](images/bc_plan.png)
***

- Thomas, Williams and Sandilands (2007)
- Different areas require different strategies
- Zig-zags, parallel lines, census
- Analysis in Distance

Sideline: alternative terminology
=================================

"A design is an algorithm for laying down samplers in the survey area"

"A realization (from that algorithm) is called a survey plan"

Len Thomas (Talk @CREEM 2004)

H-T estimation again
====================

- Can't estimate w/ H-T w/o coverage
- "Fixed" "designs" violate assumptions
  - Some animals have $\mathbb{P}(\text{included})=0$
- "Deteriorate" pooling robustness property
- What can we do?

More on variance
================

- Encounter rate variance $\approx n_j/l_j - n/L$
- Within-transect variation can be **bad**
  - e.g., N-S transect, N-S density gradient

Stratification
==============

- If we suspect density change can stratify!
- Pre or post hoc (spatial and non-spatial)


<img src="images/minke-strata.png" width="80%">



I am going to stop talking very soon
================================================
type:section


Summary
=======

- H-T is a spatial model (sort of)
- Violated an assumption if no randomness
  - Hard to assess how bad this is
- Fewster et al (2009) and Fewster (2011) give variance approaches


blah
=========
type:section
title:none

<p style="font-size:800%">Part II</p>



Spatial models
==============
type:section

Spatial models of distance sampling data
========================================

- Collect spatially referenced data
- Why not make spatially-explicit models?
- Go beyond stratified estimates
- Relate environmental covariates to counts

This is the rosy picture talk
==============================
type:section

We'll talk about the grim reality later
==============================
type:section


Example data in this talk
=========================
type:section

Sperm whales off the US east coast
====================================

<img src="images/spermwhale.png" width="100%">

***

- Hang out near canyons, eat squid
- Surveys in 2004, US east coast
- Combination of data from 2 NOAA cruises
- Thanks to Debi Palka, Lance Garrison for data. Jason Roberts for data prep.


Example data
============

<img src="images/data_ships.png">


Model formulation
=================

- Pure spatial, pure environmental, mixed?
- May have some prior knowledge
  - Biology/ecology
- What are drivers of distribution?
- Inferential aim
  - Abundance
  - Ecology

Density surface models
===============
type:section

Hedley and Buckland (2004)

Miller et al. (2013)

DSM flow diagram
==================
title:false

<img src="images/dsm-flow2.png" alt="DSM process flow diagram" width=120%>

- Ignoring group size (more on that tomorrow)

Data setup
================================================
title:false

<img src="images/dsmproc.png">

<small>[Physeter catodon by Noah Schlottman](http://phylopic.org/image/dc76cbdb-dba5-4d8f-8cf3-809515c30dbd/)</small>

How do we model that?
================================================
type:section

SPOILER ALERT: your model is probably just a very fancy GLM
================================================
type:section

Generalised additive models (in 1 slide)
================================================

Taking the previous example...

$$
\mathbb{E}\left(n_j\right) = \color{red}{A_j}\color{blue}{\hat{p}_j} \color{green}{\exp}\left[\color{grey}{ \beta_0 + \sum_k s_k(z_{kj})} \right]
$$

$n_j\sim$ some count distribution

- $\color{red}{\text{area of segment}}$
- $\color{blue}{\text{probability of detection in segment}}$
- $\color{green}{\text{(inverse) link function}}$
- $\color{grey}{\text{model terms}}$

What about those s thingys?
================================================
type:section

Covariates
==========

- space, time, environmental (remotely sensed?) data

```{r fig.width=6}
p <- ggplot(predgrid) + geom_tile(aes(x=x, y=y, fill=Depth)) +
  sp_theme + scale_fill_viridis() +
  coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us
print(p)
p <- ggplot(predgrid) + geom_tile(aes(x=x, y=y, fill=NPP)) +
  sp_theme + scale_fill_viridis() +
  coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us
print(p)
```


s thingys
================================================
title:false

```{r seconddsm, echo=FALSE, fig.height=11, fig.width=16}
dsm_env_tw <- dsm(count~s(Depth) + s(NPP) + s(SST),
                  ddf.obj=df_hr,
                segment.data=segs, observation.data=obs,
                family=tw(), method="REML")
opar <- par(mfrow=c(2,2), mar=c(4, 6, 4, 2) + 0.1, cex.lab=2.5, cex.axis=2.5)
plot(dsm_env_tw, select=1, scale=0, lwd=3, shade=TRUE)
plot(dsm_env_tw, select=2, scale=0, lwd=3, shade=TRUE)
plot(dsm_env_tw, select=3, scale=0, lwd=3, shade=TRUE)
plot(0:10,0:10, type="n", axes=FALSE)
text(label="Look at the\ny-axis scale!", cex=5, x=5, y=5)
par(opar)
```

Modelling smooths
==================

- 1-dimension: not much difference
- 2D more tricky
  - edge effects
  - tricky boundaries 
  - more tomorrow
- Now going to do some maths...
  - (ignore at will)

How do we build them?
========================
left: 55%

```{r results='hide', out.width='\\textwidth', fig.width=10, fig.height=10}
set.seed(2)
dat <- gamSim(1,n=400,dist="normal",scale=2)
b <- gam(y~s(x0, k=5, bs="cr"),data=dat)

# main plot
plot(b, se=FALSE, ylim=c(-1, 1), lwd=3, asp=1/2)

# plot each basis
cf <- coef(b)
xp <- data.frame(x0=seq(0, 1, length.out=100))
Xp <- predict(b, newdata=xp, type="lpmatrix")

for(i in 1:length(cf)){
  cf_c <- cf
  cf_c[-i] <- 0
  cf_c[i] <- 1
  lines(xp$x0, as.vector(Xp%*%cf_c), lty=i+1, lwd=2)
}
```

*** 

- Functions made of other, simpler functions
- **Basis functions**, $b_k$
- Estimate $\beta_k$ 
- $s(x) = \sum_{k=1}^K \beta_k b_k(x)$


Straight lines vs. interpolation
=================================

```{r wiggles, out.width='\\textwidth', fig.width=10, fig.height=10}
library(mgcv)
# hacked from the example in ?gam
set.seed(2) ## simulate some data... 
dat <- gamSim(1,n=50,dist="normal",scale=0.5, verbose=FALSE)
dat$y <- dat$f2 + rnorm(length(dat$f2), sd = sqrt(0.5))
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10-mean(dat$y)
ylim <- c(-4,6)

# fit some models
b.justright <- gam(y~s(x2),data=dat)
b.sp0 <- gam(y~s(x2, sp=0, k=50),data=dat)
b.spinf <- gam(y~s(x2),data=dat, sp=1e10)

curve(f2,0,1, col="blue", ylim=ylim, lwd=2)
points(dat$x2, dat$y-mean(dat$y), pch=18, cex=1.3)

```
***
- Want a line that is "close" to all the data
- Don't want interpolation -- we know there is "error"
- Balance between **interpolation** and **generality**

How wiggly is a function?
=========================

```{r wigglyanim, results="hide"}
library(numDeriv)
library(animation)
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10 - mean(dat$y)

xvals <- seq(0,1,len=100)

plot_wiggly <- function(f2, xvals){

  # pre-calculate
  f2v <- f2(xvals)
  f2vg <- grad(f2,xvals)
  f2vg2 <- unlist(lapply(xvals, hessian, func=f2))
  f2vg2min <- min(f2vg2) -2
  
  # now plot
  for(i in 1:length(xvals)){
    par(mfrow=c(1,3))
    plot(xvals, f2v, type="l", main="function", ylab="f")
    points(xvals[i], f2v[i], pch=19, col="red")
    
    plot(xvals, f2vg, type="l", main="derivative", ylab="df/dx")
    points(xvals[i], f2vg[i], pch=19, col="red")
    
    plot(xvals, f2vg2, type="l", main="2nd derivative", ylab="d2f/dx2")
    points(xvals[i], f2vg2[i], pch=19, col="red")
    polygon(x=c(0,xvals[1:i], xvals[i],f2vg2min),
            y=c(f2vg2min,f2vg2[1:i],f2vg2min,f2vg2min), col = "grey")
    
    ani.pause()
  }
}

saveGIF(plot_wiggly(f2, xvals), "wiggly.gif", interval = 0.2, ani.width = 800, ani.height = 400)
```

![Animation of derivatives](wiggly.gif)

Making wigglyness matter
=========================

- Fit needs to be **penalised**
- *Something* like:

$$
\int_\mathbb{R} \left( \frac{\partial^2 s(x)}{\partial x^2}\right)^2 \text{d}x\\
$$

- (Can always re-write this in the form $\mathbf{\beta}^T \mathbf{S} \mathbf{\beta}$)
- Estimate the $\beta_k$ terms but penalise objective
  - "closeness to data" + penalty (REML/ML)



Smoothing parameter
=======================


```{r wiggles-plot, fig.width=15}
# make three plots, w. estimated smooth, truth and data on each
par(mfrow=c(1,3), cex.main=3.5, cex.lab=2)

plot(b.justright, se=FALSE, ylim=ylim, main=expression(lambda*plain("= just right")), lwd=1.5)
points(dat$x2, dat$y-mean(dat$y), cex=1.5, pch=19)
curve(f2,0,1, col="blue", add=TRUE, lwd=1.5)

plot(b.sp0, se=FALSE, ylim=ylim, main=expression(lambda*plain("=")*0), lwd=1.5)
points(dat$x2, dat$y-mean(dat$y), cex=1.5, pch=19)
curve(f2,0,1, col="blue", add=TRUE, lwd=1.5)

plot(b.spinf, se=FALSE, ylim=ylim, main=expression(lambda*plain("=")*infinity), lwd=1.5) 
points(dat$x2, dat$y-mean(dat$y), cex=1.5, pch=19)
curve(f2,0,1, col="blue", add=TRUE, lwd=1.5)
```


Sideline: GAMs are Bayesian models
=================================

- Generally:
  - penalties are improper prior precision matrices
  - (nullspace gives improper priors)
- Using shrinkage smoothers:  
  - *proper* priors
  - empirical Bayes interpretation



Beyond univariate smooths?
================================================
left:60%


```{r tensor, echo=FALSE, results='hide', fig.width=8, fig.height=6, messages=FALSE, warning=FALSE}
dsm_xy_tw <- dsm(count~ ti(x,y) + ti(x) + ti(y),
                  ddf.obj=df_hr,
                segment.data=segs, observation.data=obs,
                family=tw(), method="REML")

library(mgcv)

layout(matrix(c(1,2,3,3), 2, 2), widths=c(1.5,1.5,1), height=c(1,1,2))
opar <- par(mar=c(4, 3, 1, 2) + 0.1)
plot(dsm_xy_tw, select=3)
plot(dsm_xy_tw, select=2)
par(mar=c(0, 0, 0, 0) + 0.1)
vis.gam(dsm_xy_tw, view=c("x","y"), theta=-60, phi=30, color="bw")
par(opar)
```

***

- Can build (*anisotropic*) **tensor product** terms
- Take 2 or more univariate terms
- Thin plate regression splines allow multivariate terms (*isotropic*)

Spatial smoothing
=================

```{r, out.width='\\textwidth', fig.width=10, fig.height=10}
dsm_xy <- dsm(count~s(x,y),
                  ddf.obj=df_hr,
                  segment.data=segs, observation.data=obs,
                  family=tw())

predgrid$Nhat <- predict(dsm_xy, predgrid)
p <- ggplot(predgrid) + 
      geom_tile(aes(x=x, y=y, fill=Nhat, width=10*1000, height=10*1000)) +
      geom_point(aes(x=x, y=y), data=dsm_xy$data[dsm_xy$data$count>0,]) +
      sp_theme +
      coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us +
      labs(fill="Density")+
      scale_fill_viridis()
print(p)
```

***

- Can just smooth in space
- Valid abundance estimation technique
- Useful for EDA for env. cov. models (**hard** day 2!)
- Not good for extrapolations
- Basis choice can matter!

Why GAMs are cool...
================================================
![](images/igam.jpg)
***
- Fancy smooths (cyclic, boundaries, ...)
- Fancy responses (exp family and beyond!)
- Random effects (by equivalence)
- Markov random fields
- Correlation structures
- See Wood (2006/2017) for a handy intro


Let's fit a model
================================================

```{r firstdsm, echo=TRUE}
library(dsm)
# environmental covariates
dsm_env_tw <- dsm(count~s(Depth) + s(NPP) + s(SST),
                  ddf.obj=df_hr,
                  segment.data=segs, observation.data=obs,
                  family=tw())
# space
dsm_xy_tw <- dsm(count~s(x, y), ddf.obj=df_hr,
                segment.data=segs, observation.data=obs,
                family=tw())
```

`dsm` is based on `mgcv` by Simon Wood

<img src="images/mgcv-inside.png" align="right">



Simple! Done?
=============
type:section

blah
=========
type:section
title:none

<p style="font-size:1000%">NO</p>

More on model checking later...
================================
type:section


Predictions/abundance estimates
===============================

```{r predplot, out.width='\\textwidth', fig.width=10, fig.height=10}
predgrid$Nhat <- predict(dsm.tw.xy, predgrid)
p <- ggplot(predgrid) + 
      geom_tile(aes(x=x, y=y, fill=Nhat, width=10*1000, height=10*1000)) +
      geom_point(aes(x=x, y=y), data=dsm_xy$data[dsm_xy$data$count>0,]) +
      sp_theme +
      coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us +
      labs(fill="Density")+
      scale_fill_viridis()
print(p)
```

***

- Grid of covariates must be available
  - Predict within survey area
  - Extrapolate outside (with caution)
- Working on a grid of cells
- Plot is `s(x,y) + s(Depth)`
- Add up to get abundance


Estimating variance
===================

- Uncertainty from:
  - detection function parameters
  - spatial model
- Need to propagate uncertainty!
  - Methods in `dsm`
  - Bravington, Hedley & Miller (in prep)


Plotting uncertainty
================================================

```{r, vars, echo=FALSE, out.width='\\textwidth', fig.width=10, fig.height=10}
predgrid$width <- predgrid$height <- 10*1000
predgrid_split <- split(predgrid, 1:nrow(predgrid))
var_map <- dsm.var.prop(dsm_xy_tw, predgrid_split, 
                        off.set=predgrid$off.set)
p <- plot(var_map, observations=FALSE, plot=FALSE) + 
      sp_theme +
      coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us +
      scale_fill_viridis()
print(p)
```

***

- Maps of coefficient of variation
- CV for given stratum (better)
- Visualisation is **hard**

Communicating uncertainty
================================================
<img src="images/uncanimation.gif" width="100%">

***

- Are animations a good way to do this?
- Simulate from posterior parameter distribution
- $\boldsymbol{\beta} \sim N(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\Sigma}})$
- Some features (e.g. shelf, N-S gradient) stick out

I am going to stop talking very soon
================================================
type:section

Summary
=======

- Build models in stages (detection function + GAM)
- Counts are functions of covariates
  - Pure spatial models
  - Environmental covariate models
  - Mix?!
- Fit/check using `dsm`
- Most of the theory is resolved, applications are hard


blah
=========
type:section
title:none

<p style="font-size:800%">Part III</p>


H-T or spatial or give up?
==========================
type:section


Spatial models can help
=======================

- Spatial modelling can give ubiased abundance ests
  - even with uneven coverage
  - limits to extrapolation
- V. even coverage => HT?
- "Evenness" subtle, detectability effect
  - e.g., weather bad in east


Weather or distribution?
========================

- Weather has a big effect on detectability
- Need to record during survey
- Disambiguate between distribution/detectability
- Potential confounding can be *BAD*

<img src="images/weather_or_density.png" width="100%">


Visibility during POWER 2014
=============================

<img src="images/power-weather.png" width="100%">

Thanks to Hiroto Murase and co for this data!

Covariates can make a big difference!
=====================================

<img src="images/covar-inout.png" width="100%">


Other stuff
===========
type:section


Spatial modelling won't solve all yr problems
=============================================

<img src="images/design-zzl.png" width="100%">

***
- Design issues
  - Ludicrous extrapolation
  - Survey plan not robust to weather issues
  - Non-uniform distribution wrt sampler
  - Migration
  
**Spatial models alone can't solve these issues**

Spatial modelling won't solve all yr problems
=============================================

<img src="images/ww-crap.png" width="100%">

***

- Violations of survey procedure
  - Following animals
  - Responsive movement
  - Guarding the trackline
  - Group size estimation
  
**Spatial models alone can't solve these issues**

Spatial modelling won't solve all yr problems
=============================================

- Detection functions
  - Not enough observations
  - Uncertain species ID
  - Group size

**Spatial models alone can't solve these issues**


Garbage in, garbage out
=======================
type:section
title:none

<img src="images/kitabet_2016-Nov-22.jpg" width="100%">

[@kitabet](http://twitter.com/kitabet)


Should everything be spatial?
=============================

- Do you have enough observations?
- If they do look good (even coverage, etc)
- Is it worth re-analysing from H-T?
- Point estimates similar?
- Variance may well be different?

I am going to stop talking very soon
================================================
type:section


Summary
=======

- Spatial models don't solve all problems
- Complex models can lead to complex issues
- Recording weather conditions is important
- You can always give up!

blah
=========
type:section
title:none

<p style="font-size:850%">Part IV</p>


Testing designs
===============
type:section


What can we do?
===============

- Take a survey and simulate
- Is H-T robust?
- How do different spatial models compare?
  - Only thinking about total abundance & CV

Software
========

- `ltdesigntester` [github.com/dill/ltdesigntester](http://github.com/dill/ltdesigntester)
- (based on `DSsim` by Laura Marshall, CREEM)
- Setup simulations, test what can be done
- Most of the work needs to be done in GIS
  - Survey shapefiles, covariates etc
  - Import to R, runs models, shows output

Setting up a survey simulation
==============================

<img src="images/setup-flow.png" width="100%">

Density
=======

<img src="images/denstiy-fig.png" width="100%">

***

- Grid in polygon of study area
- Either specify simple gradient or use other tools to make complex density
- Density as grid

Design
======


<img src="images/design-zzl.png" width="100%">

***

- Generate using GIS/Distance
- Export to shapefile

Detection function
==================

<img src="images/detfct-good.png" width="100%">

***

- Functional form (half-normal, hazard-rate)
- Parameters (scale, shape)
- Truncation
- (Covariates via multiple functions, more later)

Specification to simulation
===========================

<img src="images/realization.png" width="100%">

***

- Generate multiple realizations
- Analyse each with a many models
  - Different spatial, H-T
- Compare results

Test models
===========

- Spatial smoothers
  - thin plate spline, `bs="tp"` (Wood, 2003)
  - thin plate spline with shrinkage, `bs="ts"` (Marra et al., 2011)
  - Duchon spline, `bs="ds", m=c(1, 0.5)` (Miller et al., 2014)
  - tensor of thin plate spline (w/ and w/o rotated covariates)
- Stratified estimates
  - Horvitz-Thompson (w/ and w/o covariates)
  - stratified Horvitz-Thompson (w/ and w/o covariates)

Comparing performance
=====================

<img src="images/self-confidence.png" width="100%">

Important caveats
=================

- No model checking 
- Dependent on "good" detection specs
- No group size model
- No $g(0)$ or availability

Quick example code
==================

```{r eval=FALSE, echo=TRUE}
library(ltdesigntester)
# setup a simulation
my_sim <- build_sim(design_path="path/to/shp",
                    dsurf=density_surface_matrix,
                    n_grid_x=dsurf_dim_x,
                    n_grid_y=dsurf_dim_y,
                    n_pop=true_N,
                    df=detection_function_specs,
                    region="path/to/shp")
# run it!
res <- do_sim(nsim=number_of_sims,
              scenario=my_sim,
              pred_dat=prediction_data_frame, ...)
```

We made a big deal about weather earlier...
===========================================

- We can add covariates too (a wee bit unwieldy at the moment)
- Build multiple detection functions/sims in `list()`
- Covariates vary according to:
  - logit function E-W (can set pars, 2 state)
  - set values in segment data (already observed)



I am going to stop talking very soon
================================================
type:section

Summary
=======

- We can test multiple (simple) scenarios
  - Assumption of simple gradients
  - Models likely won't work for difficult stuff if they don't work for simple things
- What will work/what won't
- Simple summary plots
- Better than the rest $\neq$ good


blah
=========
type:section
title:none

<p style="font-size:800%">Part V</p>


Model checking for DSMs
=======================
type:section


Model checking
=============

- Count distribution
- Basis complexity
- Model (term) selection
- Sensitivity
- Observed vs. expected
- Cross-validation (replicability)

(Plus all the usual stuff for detection functions!)
===================================================
type:section


Count distributions
=====================

```{r countshist}
hist(dsm.nb.xy$data$count, xlab="Count", main="")
```
***
- Response is a count
- Often, it's mostly zero
- Aggregations occur at scales smaller than spatial model
- Want response distribution that deals with that
- Could mess-up variance if ignored
- Linked to segmenting
- Flexible mean-variance relationship

Negative binomial
==================

```{r negbin}
y<-seq(1,12,by=1)
disps <- seq(0.001, 1, len=10)

fymat <- matrix(NA, length(y), length(disps))

i <- 1
for(disp in disps){
  fymat[,i] <- dnbinom(y, size=disp, mu=5)
  i <- i+1
}

plot(range(y), range(fymat), type="n", ylab="Density", xlab="x", cex.lab=1.5,
     main="")

rr <- brewer.pal(8,"Dark2")

for(i in 1:ncol(fymat)){
  lines(y, fymat[,i], type="l", col=rr[i], lwd=2)
}
```
***
- $\text{Var}\left(\text{count}\right) =$ $\mathbb{E}(\text{count}) + \kappa \mathbb{E}(\text{count})^2$
- Estimate $\kappa$
- Is quadratic relationship a "strong" assumption?
- Similar to Poisson: $\text{Var}\left(\text{count}\right) =\mathbb{E}(\text{count})$ 



Tweedie distribution
=====================

```{r tweedie}

# tweedie
y<-seq(0.01,5,by=0.01)
pows <- seq(1.2, 1.9, by=0.1)

fymat <- matrix(NA, length(y), length(pows))

i <- 1
for(pow in pows){
  fymat[,i] <- dtweedie( y=y, power=pow, mu=2, phi=1)
  i <- i+1
}

plot(range(y), range(fymat), type="n", ylab="Density", xlab="x", cex.lab=1.5,
     main="")

rr <- brewer.pal(8,"Dark2")

for(i in 1:ncol(fymat)){
  lines(y, fymat[,i], type="l", col=rr[i], lwd=2)
}
```
***
-  $\text{Var}\left(\text{count}\right) = \phi\mathbb{E}(\text{count})^q$
- Common distributions are sub-cases:
  - $q=1 \Rightarrow$ Poisson
  - $q=2 \Rightarrow$ Gamma
  - $q=3 \Rightarrow$ inverse-Gaussian
- We are interested in $1 < q < 2$ 
- (here $q = 1.2, 1.3, \ldots, 1.9$)

Basis complexity
================

- Before: $s(x) = \sum_{k=1}^K \beta_k b_k(x)$
- How big should `k` be? "Big enough"
- Penalty takes care of the rest
- `?gam.check` gives useful output
  - (also residual checks etc)

gam.check text output
=====================

```{r gc, echo=TRUE, fig.keep="none"}
gam.check(dsm_env_tw)
```


Tobler's first law of geography
==================================
type:section

"Everything is related to everything else, but near things are more related than distant things"

Tobler (1970)

Implications of Tobler's law
==============================

```{r pairrrrs, fig.width=12}
plot(segs[,c("x","y","SST","EKE","NPP","Depth")], pch=19, cex=0.4)
```

What can we do about this?
===========================

- Careful inclusion of terms
- Test for sensitivity (lots of models)
- Fit models using robust criteria (REML)
- Test for concurvity (`mgcv::concurvity`, `dsm::vis.concurvity`)

<br/>

![](images/remlgcv.png)
                
Term selection
==============

- (approximate) $p$ values (Marra & Wood, 2012)
  - path dependence issues
- shrinkage methods (Marra & Wood, 2011)
- ecological-level term selection
  - *which* biomass measure?
  - include spatial smooth or not?
  
  
Observed vs. expected
=====================

- Diagnostic -- compare observed vs. expected counts
  - Compare for different covariate/aggregations
- In next `dsm`, `obs_exp()` does this
- Going back to those rough POWER models...

```
> obs_exp(b, "beaufort")
               1        2       34
Observed 3.00000 10.00000 80.00000
Expected 6.97715 12.42649 83.03773

> obs_exp(b_nc, "beaufort")
                1        2       34
Observed 3.000000 10.00000 80.00000
Expected 8.478759 17.00705 73.23535
```

Cross-validation
================

- How well does the model reproduce what we saw?
- Leave out one area, re-fit model, predict to new data
- Wenger & Olden (2012) have good spatial examples

Cross-validation example
========================

![](images/aftt-effort.png)

Cross-validation example
========================

![](images/kogia-cv.png)


I am going to stop talking very soon
================================================
type:section


2 (or more)-stage models
===========================

- Not "cool" (statistically), but...
- Multi-stage models are handy!
- Understand and **check** each part
- Split your modelling efforts amongst people



Conclusions
====================

- This methodology is general
  - Bears, birds, beer cans, Loch Ness monsters...
- Models are flexible!
  - Linear things, smooth things, random effect things (and *more*)
- If you know GLMs, you can get started with DSMs
  - Mature theoretical basis, still lots to do
- Active user community, active software development


Resources
==============

![](images/mee-paper.png)

[distancesampling.org/R/](http://distancesampling.org/R/)

[distancesampling.org/workshops/duke-spatial-2015/](http://distancesampling.org/workshops/duke-spatial-2015/)


Thanks!
===============
type:section

Slides w/ references available at converged.yt

References
==========

<div style="font-size:45%">
Fewster, R.M., Buckland, S.T., Burnham, K.P., Borchers, D.L., Jupp, P.E., Laake, J.L., et al. (2009) Estimating the Encounter Rate Variance in Distance Sampling. Biometrics, 65, 225–236.
<br/>
Fewster, R. M. (2011), Variance Estimation for Systematic Designs in Spatial Surveys. Biometrics, 67: 1518–1531.
<br/>
Hedley, S. L., & Buckland, S. T. (2004). Spatial models for line transect sampling. Journal of Agricultural, Biological, and Environmental Statistics, 9(2).
<br/>
Marques, T. A., Thomas, L., Fancy, S. G., & Buckland, S. T. (2007). Improving estimates of bird density using multiple-covariate distance sampling. The Auk, 124(4).
<br/>
Marra, G., & Wood, S. N. (2011). Practical variable selection for generalized additive models. Computational Statistics and Data Analysis, 55(7). 
<br/>
Marra, G., & Wood, S. N. (2012). Coverage Properties of Confidence Intervals for Generalized Additive Model Components. Scandinavian Journal of Statistics, 39(1).
<br/>
Wenger, S.J. and Olden, J.D. (2012) Assessing transferability of ecological models: an underappreciated aspect of statistical validation. Methods in Ecology and Evolution, 3, 260–267.


Handy awkward question answers
===============================
type:section

Don't throw away your residuals!
=================================
type:section


gam.check
=============

```{r echo=FALSE, results="hide"}
gam.check(dsm_xy_tw)
```


rqgam.check (Dunn and Smyth, 1996)
=============

```{r echo=FALSE}
rqgam.check(dsm_xy_tw)
```


Penalty matrix
===============

- For each $b_k$ calculate the penalty
- Penalty is a function of $\beta$
  - $\lambda \beta^\text{T}S\beta$
- $S$ calculated once
- smoothing parameter ($\lambda$) dictates influence


How wiggly are things?
========================

- We can set **basis complexity** or "size" ($k$)
  - Maximum wigglyness
- Smooths have **effective degrees of freedom** (EDF)
- EDF < $k$
- Set $k$ "large enough"


Let's talk about detectability
========================================================
type:section

Detectability
========================================================

<img src="images/distance-animation.gif" width=900px>

Distance sampling
========================================================

- "Fit to the histogram"
- Model:

$$
\mathbb{P} \left[ \text{animal detected } \vert \text{ animal at distance } y\right] = g(y;\boldsymbol{\theta})
$$

- Calculate the average probability of detection:

$$
\hat{p} = \frac{1}{w} \int_0^w g(y; \boldsymbol{\hat{\theta}}) \text{d}y
$$


Distance sampling (extensions)
========================================================

  * Covariates that affect detectability (Marques et al, 2007)
  * Perception bias ($g(0)<1$) (Burt et al, 2014)
  * Availability bias (Borchers et al, 2013)
  * Detection function formulations (Miller and Thomas, 2015)
  * Measurement error (Marques, 2004)
&nbsp;
<div align="center"><img src="images/covar-df.png" width=780px></div>

<small>Figure from Marques et al (2007)</small>


That's not really how the ocean works...
=========================================
type:section

Availability
================================================
type:section

We can only see whales at the surface
================================================

- What proportion of the time are they there?
  - Acoustics
  - Tags (DTAGs etc)
  - Behavioural studies
- Fixed correction to $\hat{p}$?
- Model via fancy Markov models (Borchers et al, 2013)

![](images/poles_apart_gambolling.jpg)

<small>Picture from University of St Andrews Library Special Collections</small>
