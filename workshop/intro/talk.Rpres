Overview of density surface modelling
=====================================
css: custom.css

David L Miller & Mark V Bravington

International Whaling Commission Scientific Committee 2017

```{r setup, include=FALSE}
library(knitr)
library(viridis)
opts_chunk$set(cache=TRUE, echo=FALSE)

# some useful libraries
library(dsm)
library(mgcv)

library(tweedie)
library(RColorBrewer)

# load the sperm whale data
load("spermwhale-analysis/df-models.RData")
load("spermwhale-analysis/count-models.RData")
load("spermwhale-analysis/predgrid.RData")
load("spermwhale-analysis/sperm-data.RData")

# make a theme
library(ggplot2)
library(cowplot)
sp_theme <- theme_cowplot() +
  theme(panel.grid.major = element_line(colour="grey92", size=0.25),
        axis.line=element_blank())

# add the US to maps
library(maptools)
library(rgeos)
library(maps)
library(mapdata)
coastline <- map("world", c("USA"), fill=TRUE, plot=FALSE)
coastline <- map2SpatialPolygons(coastline, ID=coastline$name,# ID=IDs,
                                 proj4string=CRS("+proj=longlat +datum=WGS84"))
# AEA projection, as in EEZ models of Roberts et al
Projection <- CRS("+proj=aea +lat_1=38 +lat_2=30 +lat_0=34 +lon_0=-73 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0")
# project!
coastline <- spTransform(coastline, Projection)
bnd <- as.data.frame(coastline@polygons[[1]]@Polygons[[1]]@coords)
colnames(bnd) <- c("x", "y")

us <- geom_polygon(aes(x=x, y=y), fill = "#1A9850",
               data=bnd)
```

Why are we interested in spatially-explicit estimation?
======================
type:section


blah
=========
type:section
title:none

<p style="font-size:800%">Part I</p>



Horvitz-Thompson estimation: the good, the bad and the ugly
===========================
type:section


Horvitz-Thompson-like estimators
========================================================

- Rescale the (flat) density and extrapolate

$$
\hat{N} = \frac{\text{study area}}{\text{covered area}}\sum_{i=1}^n \frac{s_i}{\hat{p}_i}
$$

- $s_i$ are group/cluster sizes
- $\hat{p}_i$ is the detection probability (from distance sampling)

Hidden in this formula is a simple assumption
=============================================

- Probability of sampling every point in the study area is equal
- Is this true? Sometimes.
- If (and only if) the design is randomised

Many faces of randomisation
===========================

```{r randomisation, fig.width=10, fig.height=4}
set.seed(12133)
par(mfrow=c(1,3))

# true random sample
plot(c(0, 1), c(0, 1), type="n", xlab="", ylab="", axes=FALSE, asp=1, main="random placement")
dat <- data.frame(x=runif(10), y=runif(10))
angle <- runif(10, 0, 2*pi)
len <- 0.2
arrows(dat$x, dat$y, dat$x+len*cos(angle), dat$y+len*sin(angle), length=0)
dat <- data.frame(x=runif(10), y=runif(10))
angle <- runif(10, 0, 2*pi)
len <- 0.2
arrows(dat$x, dat$y, dat$x+len*cos(angle), dat$y+len*sin(angle), length=0, col="grey40", lty=2)
box()

# parallel random offset
plot(c(0, 1), c(0, 1), type="n", xlab="", ylab="", axes=FALSE, asp=1, main="random offset parallel lines")
abline(v=seq(0, 1, len=10))
abline(v=seq(0, 1, len=10)+0.07, col="grey40", lty=2)
box()

# random offset zigzag

## make a zigzag
n_segs <- 10
zz <- data.frame(x   = c(seq(0, 0.5, len=n_segs),
                         seq(0.5, 1, len=n_segs)),
                 y   = c(seq(0, 1, len=n_segs),
                         seq(1, 0, len=n_segs)))
# many zigzags
mzz <- rbind(zz,zz,zz)
mzz$x <- mzz$x/3
ind <- 1:nrow(zz)
mzz$x[ind+nrow(zz)] <- mzz$x[ind+nrow(zz)]+1/3
mzz$x[ind+2*nrow(zz)] <- mzz$x[ind+2*nrow(zz)]+2/3

plot(mzz, type="l", xlab="", ylab="", axes=FALSE, asp=1, main="random offset zigzag")
lines(mzz$x+0.06, mzz$y, col="grey40", lty=2)
box()
```

What does this randomisation give us?
=====================================

- Coverage probability
- H-T estimator assumes even coverage
- (or you can estimate)
- Otherwise not really valid

Estimating coverage
===================

- We can estimate coverage of a non-uniform design!
- In Distance!
- Example from BC, Canada in this paper:

![Thomas paper](images/thomas-paper.png)


Estimating coverage
===================

![](images/bad_coverage.png)

A complex survey plan
=====================

![](images/bc_plan.png)
***

- Thomas, Williams and Sandilands (2007)
- Different areas require different strategies
- Zig-zags, parallel lines, census
- Analysis in Distance

Sideline: alternative terminology
=================================

"A design is an algorithm for laying down samplers in the survey area"

"A realization (from that algorithm) is called a survey plan"

Len Thomas (Talk @CREEM 2004)

H-T estimation again
====================

- Can't estimate w/ H-T w/o coverage
- "Fixed" "designs" violate assumptions
  - Some animals have $\mathbb{P}(\text{included})=0$
- "Deteriorate" pooling robustness property
- What can we do?



blah
=========
type:section
title:none

<p style="font-size:800%">Part II</p>



Spatial models
==============
type:section

Spatial models of distance sampling data
========================================

- Collect spatially referenced data
- Why not make spatially-explicit models?
- Go beyond stratified estimates
- Relate environmental covariates to counts

This is the rosey picture talk
==============================
type:section

We'll talk about the grim reality later
==============================
type:section


Example data in this talk
=========================
type:section

Sperm whales off the US east coast
====================================

<img src="images/spermwhale.png" width="100%">

***

- Hang out near canyons, eat squid
- Surveys in 2004, US east coast
- Combination of data from 2 NOAA cruises
- Thanks to Debi Palka, Lance Garrison for data. Jason Roberts for data prep.


Example data
============

<img src="images/data_ships.png">


Model formulation
=================

- Have some prior knowledge
  - Biology
  - Ecology
- What are drivers of distribution?


Density surface models
===============
type:section

Hedley and Buckland (2004)

Miller et al. (2013)

DSM flow diagram
==================
title:false

<img src="images/dsm-flow2.png" alt="DSM process flow diagram" width=120%>

Data setup
================================================
title:false

<img src="images/dsmproc.png">

<small>[Physeter catodon by Noah Schlottman](http://phylopic.org/image/dc76cbdb-dba5-4d8f-8cf3-809515c30dbd/)</small>

How do we model that?
================================================
type:section

SPOILER ALERT: your model is probably just a very fancy GLM
================================================
type:section

Generalised additive models (in 1 slide)
================================================

Taking the previous example...

$$
\mathbb{E}\left(n_j\right) = \color{red}{A_j}\color{blue}{\hat{p}_j} \color{green}{\exp}\left[\color{grey}{ \beta_0 + \sum_k s_k(z_{kj})} \right]
$$

$n_j\sim$ some count distribution

- $\color{red}{\text{area of segment}}$
- $\color{blue}{\text{probability of detection in segment}}$
- $\color{green}{\text{(inverse) link function}}$
- $\color{grey}{\text{model terms}}$

What about those s thingys?
================================================
type:section

Covariates
==========

- space, time, environmental (remotely sensed?) data

```{r fig.width=6}
p <- ggplot(predgrid) + geom_tile(aes(x=x, y=y, fill=Depth)) +
  sp_theme + scale_fill_viridis() +
  coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us
print(p)
p <- ggplot(predgrid) + geom_tile(aes(x=x, y=y, fill=NPP)) +
  sp_theme + scale_fill_viridis() +
  coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us
print(p)
```


f thingys
================================================
title:false

```{r seconddsm, echo=FALSE, fig.height=11, fig.width=16}
dsm_env_tw <- dsm(count~s(Depth) + s(NPP) + s(SST),
                  ddf.obj=df_hr,
                segment.data=segs, observation.data=obs,
                family=tw(), method="REML")
opar <- par(mar=c(4, 6, 4, 2) + 0.1, cex.lab=2.5, cex.axis=2.5)
plot(dsm_env_tw, page=1, scale=0, lwd=3, shade=TRUE)
par(opar)
```



How do we build them?
========================
left: 55%

```{r results='hide', out.width='\\textwidth', fig.width=10, fig.height=10}
set.seed(2)
dat <- gamSim(1,n=400,dist="normal",scale=2)
b <- gam(y~s(x0, k=5, bs="cr"),data=dat)

# main plot
plot(b, se=FALSE, ylim=c(-1, 1), lwd=2,asp=1/2)

# plot each basis
cf <- coef(b)
xp <- data.frame(x0=seq(0, 1, length.out=100))
Xp <- predict(b, newdata=xp, type="lpmatrix")

for(i in 1:length(cf)){
  cf_c <- cf
  cf_c[-i] <- 0
  cf_c[i] <- 1
  lines(xp$x0, as.vector(Xp%*%cf_c), lty=i+1, lwd=2)
}
```

*** 

- Functions made of other, simpler functions
- **Basis functions**, $b_k$
- Estimate $\beta_k$ 
- $s(x) = \sum_{k=1}^K \beta_k b_k(x)$


Straight lines vs. interpolation
=================================

```{r wiggles, out.width='\\textwidth', fig.width=10, fig.height=10}
library(mgcv)
# hacked from the example in ?gam
set.seed(2) ## simulate some data... 
dat <- gamSim(1,n=50,dist="normal",scale=0.5, verbose=FALSE)
dat$y <- dat$f2 + rnorm(length(dat$f2), sd = sqrt(0.5))
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10-mean(dat$y)
ylim <- c(-4,6)

# fit some models
b.justright <- gam(y~s(x2),data=dat)
b.sp0 <- gam(y~s(x2, sp=0, k=50),data=dat)
b.spinf <- gam(y~s(x2),data=dat, sp=1e10)

curve(f2,0,1, col="blue", ylim=ylim, lwd=2)
points(dat$x2, dat$y-mean(dat$y), pch=18, cex=1.3)

```
***
- Want a line that is "close" to all the data
- Don't want interpolation -- we know there is "error"
- Balance between **interpolation** and **generality**

How wiggly is a function?
=========================

```{r wigglyanim, results="hide"}
library(numDeriv)
library(animation)
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10 - mean(dat$y)

xvals <- seq(0,1,len=100)

plot_wiggly <- function(f2, xvals){

  # pre-calculate
  f2v <- f2(xvals)
  f2vg <- grad(f2,xvals)
  f2vg2 <- unlist(lapply(xvals, hessian, func=f2))
  f2vg2min <- min(f2vg2) -2
  
  # now plot
  for(i in 1:length(xvals)){
    par(mfrow=c(1,3))
    plot(xvals, f2v, type="l", main="function", ylab="f")
    points(xvals[i], f2v[i], pch=19, col="red")
    
    plot(xvals, f2vg, type="l", main="derivative", ylab="df/dx")
    points(xvals[i], f2vg[i], pch=19, col="red")
    
    plot(xvals, f2vg2, type="l", main="2nd derivative", ylab="d2f/dx2")
    points(xvals[i], f2vg2[i], pch=19, col="red")
    polygon(x=c(0,xvals[1:i], xvals[i],f2vg2min),
            y=c(f2vg2min,f2vg2[1:i],f2vg2min,f2vg2min), col = "grey")
    
    ani.pause()
  }
}

saveGIF(plot_wiggly(f2, xvals), "wiggly.gif", interval = 0.2, ani.width = 800, ani.height = 400)
```

![Animation of derivatives](wiggly.gif)

Making wigglyness matter
=========================

- Fit needs to be **penalised**
- *Something* like:

$$
\int_\mathbb{R} \left( \frac{\partial^2 s(x)}{\partial x^2}\right)^2 \text{d}x\\
$$

- (Can always re-write this in the form $\mathbf{\beta}^T \mathbf{S} \mathbf{\beta}$)
- Estimate the $\beta_k$ terms but penalise objective
  - "closeness to data" + penalty (REML/ML)



Smoothing parameter
=======================


```{r wiggles-plot, fig.width=15}
# make three plots, w. estimated smooth, truth and data on each
par(mfrow=c(1,3), cex.main=3.5, cex.lab=2)

plot(b.justright, se=FALSE, ylim=ylim, main=expression(lambda*plain("= just right")), lwd=1.5)
points(dat$x2, dat$y-mean(dat$y), cex=1.5, pch=19)
curve(f2,0,1, col="blue", add=TRUE, lwd=1.5)

plot(b.sp0, se=FALSE, ylim=ylim, main=expression(lambda*plain("=")*0), lwd=1.5)
points(dat$x2, dat$y-mean(dat$y), cex=1.5, pch=19)
curve(f2,0,1, col="blue", add=TRUE, lwd=1.5)

plot(b.spinf, se=FALSE, ylim=ylim, main=expression(lambda*plain("=")*infinity), lwd=1.5) 
points(dat$x2, dat$y-mean(dat$y), cex=1.5, pch=19)
curve(f2,0,1, col="blue", add=TRUE, lwd=1.5)
```

Beyond univariate smooths?
================================================
left:60%


```{r tensor, echo=FALSE, results='hide', fig.width=8, fig.height=6, messages=FALSE, warning=FALSE}
dsm_xy_tw <- dsm(count~ ti(x,y) + ti(x) + ti(y),
                  ddf.obj=df_hr,
                segment.data=segs, observation.data=obs,
                family=tw(), method="REML")

library(mgcv)

layout(matrix(c(1,2,3,3), 2, 2), widths=c(1.5,1.5,1), height=c(1,1,2))
opar <- par(mar=c(4, 3, 1, 2) + 0.1)
plot(dsm_xy_tw, select=3)
plot(dsm_xy_tw, select=2)
par(mar=c(0, 0, 0, 0) + 0.1)
vis.gam(dsm_xy_tw, view=c("x","y"), theta=-60, phi=30, color="bw")
par(opar)
```

***

- Can build (*anisotropic*) **tensor product** terms
- Take 2 or more univariate terms
- Thin plate regression splines allow multivariate terms (*isotropic*)

Traditional spatial smoothing
=============================
```{r, out.width='\\textwidth', fig.width=10, fig.height=10}
dsm_xy <- dsm(count~s(x,y),
                  ddf.obj=df_hr,
                  segment.data=segs, observation.data=obs,
                  family=tw())

predgrid$Nhat <- predict(dsm_xy, predgrid)
p <- ggplot(predgrid) + 
      geom_tile(aes(x=x, y=y, fill=Nhat, width=10*1000, height=10*1000)) +
      geom_point(aes(x=x, y=y), data=dsm_xy$data[dsm_xy$data$count>0,]) +
      sp_theme +
      coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us +
      labs(fill="Density")+
      scale_fill_viridis()
print(p)
```

***

- Can just smooth in space
- Often a good starting place
- Does this give us much more information? (sometimes?)
- Generally not good for extrapolations though

Why GAMs are cool...
================================================
![](images/igam.jpg)
***
- Fancy smooths (cyclic, boundaries, ...)
- Fancy responses (exp family and beyond!)
- Random effects (by equivalence)
- Markov random fields
- Correlation structures
- See Wood (2006/2017) for a handy intro


Let's fit a model
================================================

```{r firstdsm, echo=TRUE}
library(dsm)
# environmental covariates
dsm_env_tw <- dsm(count~s(Depth) + s(NPP) + s(SST),
                  ddf.obj=df_hr,
                  segment.data=segs, observation.data=obs,
                  family=tw())
# space
dsm_xy_tw <- dsm(count~s(x, y), ddf.obj=df_hr,
                segment.data=segs, observation.data=obs,
                family=tw())
```

`dsm` is based on `mgcv` by Simon Wood

<img src="images/mgcv-inside.png" align="right">



Simple! Done?
=============
type:section

blah
=========
type:section
title:none

<p style="font-size:1000%">NO</p>

More on model checking later...
================================
type:section


Predictions over arbitrary areas
===================================

```{r predplot, out.width='\\textwidth', fig.width=10, fig.height=10}
predgrid$Nhat <- predict(dsm.tw.xy, predgrid)
p <- ggplot(predgrid) + 
      geom_tile(aes(x=x, y=y, fill=Nhat, width=10*1000, height=10*1000)) +
      geom_point(aes(x=x, y=y), data=dsm_xy$data[dsm_xy$data$count>0,]) +
      sp_theme +
      coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us +
      labs(fill="Density")+
      scale_fill_viridis()
print(p)
```

***

- Grid of covariates must be available
  - Predict within survey area
  - Extrapolate outside (with caution)
- Working on a grid of cells
- Plot is `s(x,y) + s(Depth)`


Estimating variance
===================

- Uncertainty from:
  - detection function parameters
  - spatial model
- Need to propagate uncertainty!
  - Methods in `dsm`
  - Bravington, Hedley & Miller (in prep)


Plotting uncertainty
================================================

```{r, vars, echo=FALSE, out.width='\\textwidth', fig.width=10, fig.height=10}
predgrid$width <- predgrid$height <- 10*1000
predgrid_split <- split(predgrid, 1:nrow(predgrid))
var_map <- dsm.var.prop(dsm_xy_tw, predgrid_split, 
                        off.set=predgrid$off.set)
p <- plot(var_map, observations=FALSE, plot=FALSE) + 
      sp_theme +
      coord_equal(xlim=range(predgrid$x), ylim=range(predgrid$y)) + us +
      scale_fill_viridis()
print(p)
```

***

- Maps of coefficient of variation
- CV for given stratum (better)
- Visualisation is **hard**

Communicating uncertainty
================================================
<img src="images/uncanimation.gif" width="100%">

***

- Are animations a good way to do this?
- Simulate from posterior parameter distribution
- $\boldsymbol{\beta} \sim N(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\Sigma}})$
- Some features (e.g. shelf, N-S gradient) stick out



blah
=========
type:section
title:none

<p style="font-size:800%">Part III</p>


Spatial modelling is not a panacea
==================================
type:section



Spatial modelling won't solve all yr problems
=============================================

- Design issues
  - Biased effort 
  - Effort parallel to density gradient
  - Tracklines not robust to weather issues
  
**Spatial models can't solve these issues**

Spatial modelling won't solve all yr problems
=============================================

<img src="images/ww-crap.png" width="100%">

***

- Violations of survey procedure
  - Following animals
  - Effort parallel to density gradient
  - Responsive movement
  - Guarding the trackline
  
**Spatial models can't solve these issues**

Garbage in, garbage out
=======================
type:section
title:none

<img src="images/kitabet_2016-Nov-22.jpg" width="100%">


Weather, modelling and you
==========================

- Weather has a big effect on detectability
- Need to record during survey
- Disambiguate between distribution/detectability
- Potential confounding can be *BAD*

Visibility during POWER 2014
=============================

<img src="images/power-weather.png" width="100%">

Thanks to Hiroto Murase and co for this data!

Detectability during POWER 2014 (rudimentary analysis)
======================================================
title:none

<img src="images/power_df.png" width="100%">

Two models, two pictures
========================

<img src="images/covar-inout.png">

This effect gets worse w/ confounding
=====================================

DIAGRAM

blah
=========
type:section
title:none

<p style="font-size:850%">Part IV</p>


Testing designs
==================================
type:section


What can we do?
===============

- Take a survey and simulate
- Is H-T robust?
- How do different spatial models compare?
- **if** the distribution is simple

Software
========

- `ltdesigntester` [github.com/dill/ltdesigntester](http://github.com/dill/ltdesigntester)
- (based on `DSsim` by Laura Marshall, CREEM)
- Setup simulations, test what can be done
- Most of the work needs to be done in GIS
  - Survey shapefiles, covariates etc
  - Import to R, runs models, shows output

Setting up a survey simulation
==============================

<img src="images/setup-flow.png" width="100%">

Density
=======

<img src="images/density-fig.png" width="100%">

***

- Grid in polygon of study area
- Either specify simple gradient or use other tools to make complex density
- Modifying matrix of values

Design
======


<img src="images/design-zzl.png" width="100%">

***

- Generate using GIS/Distance
- Export to shapefile

Detection function
==================

<img src="images/detfct-good.png" width="100%">

***

- Functional form (half-normal, hazard-rate)
- Parameters (scale, shape)
- Truncation
- (Covariates via multiple functions, more later)

Specification to simulation
===========================

<img src="images/realization.png" width="100%">

***

- Generate multiple realizations
- Analyse each with a many models
- Compare results

Test models
===========

- Spatial smoothers
  - thin plate spline, `bs="tp"` (Wood, 2003)
  - thin plate spline with shrinkage, `bs="ts"` (Marra et al., 2011)
  - Duchon spline, `bs="ds", m=c(1, 0.5)` (Miller et al., 2014)
  - tensor of thin plate spline (w/ and w/o rotated covariates)
- Stratified estimates
  - Horvitz-Thompson (w/ and w/o covariates)
  - stratified Horvitz-Thompson (w/ and w/o covariates)

Comparing performance
=====================

<img src="images/self-confidence.png" width="100%">

Important caveats
=================

- No model checking 
- Dependent on "good" detection specs
- No group size model
- No $g(0)$ or availability


blah
=========
type:section
title:none

<p style="font-size:800%">Part V</p>


Model checking for DSMs
=======================
type:section



Model checking
=============

- Response distribution
- Model (term) selection
- Sensitivity
- Cross-validation (replicability)

Count distributions
=====================

```{r countshist}
hist(dsm.nb.xy$data$count, xlab="Count", main="")
```
***
- Response is a count (not not always integer)
- Often, it's mostly zero (that's complicated)
- Want response distribution that deals with that
- Flexible mean-variance relationship

Negative binomial
==================

```{r negbin}
y<-seq(1,12,by=1)
disps <- seq(0.001, 1, len=10)

fymat <- matrix(NA, length(y), length(disps))

i <- 1
for(disp in disps){
  fymat[,i] <- dnbinom(y, size=disp, mu=5)
  i <- i+1
}

plot(range(y), range(fymat), type="n", ylab="Density", xlab="x", cex.lab=1.5,
     main="")

rr <- brewer.pal(8,"Dark2")

for(i in 1:ncol(fymat)){
  lines(y, fymat[,i], type="l", col=rr[i], lwd=2)
}
```
***
- $\text{Var}\left(\text{count}\right) =$ $\mathbb{E}(\text{count}) + \kappa \mathbb{E}(\text{count})^2$
- Estimate $\kappa$
- Is quadratic relationship a "strong" assumption?
- Similar to Poisson: $\text{Var}\left(\text{count}\right) =\mathbb{E}(\text{count})$ 



Tweedie distribution
=====================

```{r tweedie}

# tweedie
y<-seq(0.01,5,by=0.01)
pows <- seq(1.2, 1.9, by=0.1)

fymat <- matrix(NA, length(y), length(pows))

i <- 1
for(pow in pows){
  fymat[,i] <- dtweedie( y=y, power=pow, mu=2, phi=1)
  i <- i+1
}

plot(range(y), range(fymat), type="n", ylab="Density", xlab="x", cex.lab=1.5,
     main="")

rr <- brewer.pal(8,"Dark2")

for(i in 1:ncol(fymat)){
  lines(y, fymat[,i], type="l", col=rr[i], lwd=2)
}
```
***
-  $\text{Var}\left(\text{count}\right) = \phi\mathbb{E}(\text{count})^q$
- Common distributions are sub-cases:
  - $q=1 \Rightarrow$ Poisson
  - $q=2 \Rightarrow$ Gamma
  - $q=3 \Rightarrow$ Normal
- We are interested in $1 < q < 2$ 
- (here $q = 1.2, 1.3, \ldots, 1.9$)



Tobler's first law of geography
==================================
type:section

"Everything is related to everything else, but near things are more related than distant things"

Tobler (1970)

Implications of Tobler's law
==============================

```{r pairrrrs, fig.width=12}
plot(segs[,c("x","y","SST","EKE","NPP","Depth")], pch=19, cex=0.4)
```

What can we do about this?
===========================

- Careful inclusion of terms
- Test for sensitivity (lots of models)
- Fit models using robust criteria (REML/ML)
- Test for concurvity (`mgcv::concurvity`, `dsm::vis.concurvity`)

<br/>

![](images/remlgcv.png)
                
Term selection
==============

- (approximate) $p$ values (Marra & Wood, 2012)
  - path dependence issues
- shrinkage methods (Marra & Wood, 2011)
- ecological-level term selection
  - *which* biomass measure?
  - include spatial smooth or not?
  
Sideline: GAMs are Bayesian models
=================================

- Generally:
  - penalties are improper prior precision matrices
  - (nullspace gives improper priors)
- Using shrinkage smoothers:  
  - *proper* priors
  - empirical Bayes interpretation


Cross-validation
================

- How well does the model reproduce what we saw?
- Leave out one area, re-fit model, predict to new data
- Wenger & Olden (2012) have good spatial examples

Cross-validation example
========================

![](images/aftt-effort.png)

Cross-validation example
========================

![](images/kogia-cv.png)


I am going to stop talking very soon
================================================
type:section


2 (or more)-stage models
===========================

- Not "cool" (statistically), but...
- Multi-stage models are handy!
- Understand and **check** each part
- Split your modelling efforts amongst people



Conclusions
====================

- This methodology is general
  - Bears, birds, beer cans, Loch Ness monsters...
- Models are flexible!
  - Linear things, smooth things, random effect things (and *more*)
- If you know GLMs, you can get started with DSMs
  - Mature theoretical basis, still lots to do
- Active user community, active software development


Resources
==============

![](images/mee-paper.png)

[distancesampling.org/R/](http://distancesampling.org/R/)

[distancesampling.org/workshops/duke-spatial-2015/](http://distancesampling.org/workshops/duke-spatial-2015/)


Thanks!
===============
type:section

Slides w/ references available at converged.yt

References
==========

<div style="font-size:45%">
Burt, M. L., Borchers, D. L., Jenkins, K. J., & Marques, T. A. (2014). Using mark-recapture distance sampling methods on line transect surveys. Methods in Ecology and Evolution, 5(11).
<br/>
Dunn, P. K., & Smyth, G. K. (1996). Randomized Quantile Residuals. Journal of Computational and Graphical Statistics, 5(3).
<br/>
Hedley, S. L., & Buckland, S. T. (2004). Spatial models for line transect sampling. Journal of Agricultural, Biological, and Environmental Statistics, 9(2).
<br/>
Marques, T. A., Thomas, L., Fancy, S. G., & Buckland, S. T. (2007). Improving estimates of bird density using multiple-covariate distance sampling. The Auk, 124(4).
<br/>
Marra, G., & Wood, S. N. (2011). Practical variable selection for generalized additive models. Computational Statistics and Data Analysis, 55(7). 
<br/>
Marra, G., & Wood, S. N. (2012). Coverage Properties of Confidence Intervals for Generalized Additive Model Components. Scandinavian Journal of Statistics, 39(1).
<br/>
Wenger, S.J. and Olden, J.D. (2012) Assessing transferability of ecological models: an underappreciated aspect of statistical validation. Methods in Ecology and Evolution, 3, 260–267.


Handy awkward question answers
===============================
type:section

Don't throw away your residuals!
=================================
type:section


gam.check
=============

```{r echo=FALSE, results="hide"}
gam.check(dsm_xy_tw)
```


rqgam.check (Dunn and Smyth, 1996)
=============

```{r echo=FALSE}
rqgam.check(dsm_xy_tw)
```


Penalty matrix
===============

- For each $b_k$ calculate the penalty
- Penalty is a function of $\beta$
  - $\lambda \beta^\text{T}S\beta$
- $S$ calculated once
- smoothing parameter ($\lambda$) dictates influence


How wiggly are things?
========================

- We can set **basis complexity** or "size" ($k$)
  - Maximum wigglyness
- Smooths have **effective degrees of freedom** (EDF)
- EDF < $k$
- Set $k$ "large enough"


Let's talk about detectability
========================================================
type:section

Detectability
========================================================

<img src="images/distance-animation.gif" width=900px>

Distance sampling
========================================================

- "Fit to the histogram"
- Model:

$$
\mathbb{P} \left[ \text{animal detected } \vert \text{ animal at distance } y\right] = g(y;\boldsymbol{\theta})
$$

- Calculate the average probability of detection:

$$
\hat{p} = \frac{1}{w} \int_0^w g(y; \boldsymbol{\hat{\theta}}) \text{d}y
$$


Distance sampling (extensions)
========================================================

  * Covariates that affect detectability (Marques et al, 2007)
  * Perception bias ($g(0)<1$) (Burt et al, 2014)
  * Availability bias (Borchers et al, 2013)
  * Detection function formulations (Miller and Thomas, 2015)
  * Measurement error (Marques, 2004)
&nbsp;
<div align="center"><img src="images/covar-df.png" width=780px></div>

<small>Figure from Marques et al (2007)</small>


That's not really how the ocean works...
=========================================
type:section

Availability
================================================
type:section

We can only see whales at the surface
================================================

- What proportion of the time are they there?
  - Acoustics
  - Tags (DTAGs etc)
  - Behavioural studies
- Fixed correction to $\hat{p}$?
- Model via fancy Markov models (Borchers et al, 2013)

![](images/poles_apart_gambolling.jpg)

<small>Picture from University of St Andrews Library Special Collections</small>
